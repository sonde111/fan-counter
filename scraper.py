import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

# ğŸš¨ éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ† ğŸš¨
TARGET_URL = "https://weibo.com/u/1705586121"  # â† æ›¿æ¢æˆä½ çš„ç›®æ ‡ç½‘å€
FANS_SELECTOR = ".#app > div.woo-box-flex.woo-box-column.Frame_wrap_3g67Q > div.woo-box-flex.Frame_content_3XrxZ.Frame_noside1_3M1rh.Frame_noside2_1lBwY > div:nth-child(2) > main > div > div > div.woo-panel-main.woo-panel-top.woo-panel-right.woo-panel-bottom.woo-panel-left.Card_wrap_2ibWe.Card_bottomGap_2Xjqi > div.woo-box-flex.woo-box-alignStart.ProfileHeader_box1_1qC-g > div.woo-box-item-flex > div.woo-box-flex.woo-box-alignCenter.ProfileHeader_h4_gcwJi > a:nth-child(1) > span"                  # â† æ›¿æ¢æˆç½‘é¡µå…ƒç´ é€‰æ‹©å™¨

# è‡ªåŠ¨è®°å½•å½“å‰æ—¥æœŸ
today = datetime.now().strftime("%Y-%m-%d")

# å°è¯•æŠ“å–æ•°æ®
try:
    headers = {'User-Agent': 'Mozilla/5.0'}  # å‡è£…æ˜¯æµè§ˆå™¨
    response = requests.get(TARGET_URL, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # æå–ç²‰ä¸æ•°ï¼ˆå…³é”®æ­¥éª¤ï¼Œéœ€è¦æ ¹æ®å®é™…ç½‘é¡µè°ƒæ•´ï¼‰
    fan_count = soup.select_one(FANS_SELECTOR).text.strip()
    
    # å‡†å¤‡æ–°æ•°æ®
    new_data = {'æ—¥æœŸ': [today], 'ç²‰ä¸æ•°': [fan_count]}
    
    # è¯»å–æ—§æ•°æ®æˆ–åˆ›å»ºæ–°è¡¨æ ¼
    try:
        df = pd.read_csv('data.csv')
    except:
        df = pd.DataFrame(columns=['æ—¥æœŸ', 'ç²‰ä¸æ•°'])
    
    # åˆå¹¶æ–°æ•°æ®
    new_df = pd.DataFrame(new_data)
    df = pd.concat([df, new_df], ignore_index=True)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    df.to_csv('data.csv', index=False)
    
except Exception as e:
    print(f"å‡ºé”™å•¦: {e}")
